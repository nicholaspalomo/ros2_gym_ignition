seed: 1
device: "cpu"

environment:
  step_size: 0.001
  rtf: 1000.0
  steps_per_run: 1
  render: True
  num_envs: 10
  max_time: 6.0
  control_dt: 0.02
  control_target_type: "joint efforts" # "joint positions"
  initial_joint_positions: [0.03, 0.4, -0.8, -0.03, 0.4, -0.8, 0.03, -0.4, 0.8, -0.03, -0.4, 0.8]
  pid_gains:
    LF_HAA:       {p: 40., i: 0., d: 1.}
    LF_HFE:       {p: 40., i: 0., d: 1.}
    LF_KFE:       {p: 40., i: 0., d: 1.}
    LH_HAA:       {p: 40., i: 0., d: 1.}
    LH_HFE:       {p: 40., i: 0., d: 1.}
    LH_KFE:       {p: 40., i: 0., d: 1.}
    RF_HAA:       {p: 40., i: 0., d: 1.}
    RF_HFE:       {p: 40., i: 0., d: 1.}
    RF_KFE:       {p: 40., i: 0., d: 1.}
    RH_HAA:       {p: 40., i: 0., d: 1.}
    RH_HFE:       {p: 40., i: 0., d: 1.}
    RH_KFE:       {p: 40., i: 0., d: 1.}
  joint_serialization:
    - LF_HAA
    - LF_HFE
    - LF_KFE
    - LH_HAA
    - LH_HFE
    - LH_KFE
    - RF_HAA
    - RF_HFE
    - RF_KFE
    - RH_HAA
    - RH_HFE
    - RH_KFE
  model:
    type: "floating"
    base_frame: "body"
  reward:
    torque: -2e-5
    target_following: 30.0
    action_smoothness: -1.2
    body_orientation: 5.0
    symmetry: -0.4
    foot_clearance: 4.0
    gait_following: 3.0
    vertical_velocity: 3.0
    body_rates: 3.0
    body_height: 5.0
    internal_contact: -1.0
    joint_velocity: -1e-3
  body_height_target: 0.48
  buffer_length: 10
  buffer_stride: 1
  target:
    randomize_velocity: 1
    x: [-1.0, 1.0] # [m/s]
    y: [-0.5, 0.5] # [m/s]
    yaw: [-0.7, 0.7] # [m/s]
    x_fixed: 1.0 # static target velocity for ANYmal to follow
    y_fixed: 0.5
    yaw_fixed: 0.7
  gait_params:
    foot_target: 0.17
    default: # trotting
      swing_start: [0.0, 0.5, 0.5, 0.0] # LF, RF, LH, RH [s]
      swing_duration: [0.5, 0.5, 0.5, 0.5] # [s]
      stride: 0.2
  end_effector_frame_names:
    - front_left_ee
    - front_right_ee
    - rear_left_ee
    - rear_right_ee
  ppo:
    architecture:
      policy: [256, 256]
      value_net: [256, 256]
      init_scale: 0.1
      activation: "Tanh"
    algorithm:
      minibatch: 2
      epoch: 4
      gamma: 0.98
      lambda: 0.8
      entropy_coeff: 0.0
      learning_rate: 1e-3
      lr_factor: 0.999999999
      clip_param: 0.2
  sac:
    batch_size: 256
    k_epochs: 1
    gamma: 0.99
    tau: 5e-3
    target_entropy: -1.
    lr_actor: 3e-4
    lr_qnet: 3e-4
    lr_alpha: 3e-4
    lr_critic: 3e-4
    alpha: 0.2
    update_every_n: 1
    layer_dim: 256
    reward_scale: 2.
    explore_steps: 0 # 10000
  camera:
    has_rgb: false
    has_depth: false
    has_logical: false
    has_thermal: false